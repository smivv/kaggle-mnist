{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numbers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Lambda, Activation, MaxPooling2D, GlobalAveragePooling2D, Conv2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "INPUT_DIR = '../input'\n",
    "\n",
    "EMB_SIZE = 8\n",
    "N_FOLDS = 5\n",
    "SEED = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "trusted": true,
    "_uuid": "b2c9dac6a869920ff0f6a2925ce924a333d33a05",
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "\"\"\" --------------------------------- Triplet loss implementation ----------------------------------- \"\"\"\n",
    "\n",
    "\n",
    "def _all_diffs(a, b):\n",
    "    \"\"\" Returns a tensor of all combinations of a - b.\n",
    "    Args:\n",
    "        a (2D tensor): A batch of vectors shaped (B1, F).\n",
    "        b (2D tensor): A batch of vectors shaped (B2, F).\n",
    "    Returns:\n",
    "        The matrix of all pairwise differences between all vectors in `a` and in\n",
    "        `b`, will be of shape (B1, B2).\n",
    "    Note:\n",
    "        For convenience, if either `a` or `b` is a `Distribution` object, its\n",
    "        mean is used.\n",
    "    \"\"\"\n",
    "    return tf.expand_dims(a, axis=1) - tf.expand_dims(b, axis=0)\n",
    "\n",
    "\n",
    "def _cdist(a, b, metric='euclidean'):\n",
    "    \"\"\"Similar to scipy.spatial's _cdist, but symbolic.\n",
    "    The currently supported metrics can be listed as `_cdist.supported_metrics` and are:\n",
    "        - 'euclidean', although with a fudge-factor epsilon.\n",
    "        - 'sqeuclidean', the squared euclidean.\n",
    "        - 'cityblock', the manhattan or L1 distance.\n",
    "    Args:\n",
    "        a (2D tensor): The left-hand side, shaped (B1, F).\n",
    "        b (2D tensor): The right-hand side, shaped (B2, F).\n",
    "        metric (string): Which distance metric to use, see notes.\n",
    "    Returns:\n",
    "        The matrix of all pairwise distances between all vectors in `a` and in\n",
    "        `b`, will be of shape (B1, B2).\n",
    "    Note:\n",
    "        When a square root is taken (such as in the Euclidean case), a small\n",
    "        epsilon is added because the gradient of the square-root at zero is\n",
    "        undefined. Thus, it will never return exact zero in these cases.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"_cdist\"):\n",
    "        diffs = _all_diffs(a, b)\n",
    "        if metric == 'sqeuclidean':\n",
    "            return tf.reduce_sum(tf.square(diffs), axis=-1)\n",
    "        elif metric == 'euclidean':\n",
    "            return tf.sqrt(tf.reduce_sum(tf.square(diffs), axis=-1) + 1e-12)\n",
    "        elif metric == 'cityblock':\n",
    "            return tf.reduce_sum(tf.abs(diffs), axis=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'The following metric is not implemented by `_cdist` yet: {}'.format(metric))\n",
    "\n",
    "\n",
    "_cdist.supported_metrics = [\n",
    "    'euclidean',\n",
    "    'sqeuclidean',\n",
    "    'cityblock',\n",
    "]\n",
    "\n",
    "\n",
    "def _get_at_indices(tensor, indices):\n",
    "    \"\"\" Like `tensor[np.arange(len(tensor)), indices]` in numpy. \"\"\"\n",
    "    counter = tf.range(tf.shape(indices, out_type=indices.dtype)[0])\n",
    "    return tf.gather_nd(tensor, tf.stack((counter, indices), -1))\n",
    "\n",
    "\n",
    "def batch_hard_loss(features, pids, metric='euclidean', margin=0.1):\n",
    "    \"\"\"Computes the batch-hard loss from arxiv.org/abs/1703.07737.\n",
    "    Args:\n",
    "        dists (2D tensor): A square all-to-all distance matrix as given by _cdist.\n",
    "        pids (1D tensor): The identities of the entries in `batch`, shape (B,).\n",
    "            This can be of any type that can be compared, thus also a string.\n",
    "        margin: The value of the margin if a number, alternatively the string\n",
    "            'soft' for using the soft-margin formulation, or `None` for not\n",
    "            using a margin at all.\n",
    "    Returns:\n",
    "        A 1D tensor of shape (B,) containing the loss value for each sample.\n",
    "        :param margin:\n",
    "        :param features:\n",
    "        :param pids:\n",
    "        :param metric:\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"batch_hard_loss\"):\n",
    "\n",
    "        dists = _cdist(features, features, metric=metric)\n",
    "\n",
    "        pids = tf.argmax(pids, axis=1)\n",
    "\n",
    "        exp_dims0 = tf.expand_dims(pids, axis=0)\n",
    "        exp_dims1 = tf.expand_dims(pids, axis=1)\n",
    "\n",
    "        same_identity_mask = tf.equal(exp_dims1, exp_dims0)\n",
    "\n",
    "        negative_mask = tf.logical_not(same_identity_mask)\n",
    "        positive_mask = tf.logical_xor(same_identity_mask,\n",
    "                                       tf.eye(tf.shape(pids)[0], dtype=tf.bool))\n",
    "\n",
    "        furthest_positive = tf.reduce_max(dists*tf.cast(positive_mask, tf.float32), axis=1)\n",
    "        # closest_negative = tf.map_fn(lambda x: tf.reduce_min(tf.boolean_mask(x[0], x[1])),\n",
    "        #                              (dists, negative_mask), tf.float32)\n",
    "        # Another way of achieving the same, though more hacky:\n",
    "        closest_negative = tf.reduce_min(dists + 1e5*tf.cast(same_identity_mask, tf.float32), axis=1)\n",
    "\n",
    "        diff = furthest_positive - closest_negative\n",
    "        if isinstance(margin, numbers.Real):\n",
    "            diff = tf.maximum(diff + margin, 0.0)\n",
    "        elif margin == 'soft':\n",
    "            diff = tf.nn.softplus(diff)\n",
    "        elif margin is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError('The margin {} is not implemented in batch_hard_loss'.format(margin))\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def triplet_loss(labels, features):\n",
    "    # https://github.com/tensorflow/tensorflow/issues/20253\n",
    "    # from tensorflow.contrib.losses import metric_learning\n",
    "    # return metric_learning.triplet_semihard_loss(K.argmax(labels, axis=1), embeddings, margin=0.2)\n",
    "    return tf.reduce_mean(batch_hard_loss(features, labels, margin=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1    ...     pixel781  pixel782  pixel783\n0      1       0       0    ...            0         0         0\n1      0       0       0    ...            0         0         0\n2      1       0       0    ...            0         0         0\n3      4       0       0    ...            0         0         0\n4      0       0       0    ...            0         0         0\n\n[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ------------------------------------ Data loading -------------------------------------- \"\"\"\n",
    "\n",
    "# load dataframes\n",
    "df_train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "trusted": true,
    "_uuid": "e1d0eb51bd4156994731546544569b9257954c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 28\n"
     ]
    }
   ],
   "source": [
    "\"\"\" --------------------------------- Data preprocessing ----------------------------------- \"\"\"\n",
    "\n",
    "# load pixel values, reshape them to 28x28 pixels and rescale from [0, 255] to [0, 1]\n",
    "x_train = df_train.iloc[:,1:].values.astype('float32') / 255.\n",
    "x_test = df_test.values.astype('float32') / 255.\n",
    "\n",
    "# make images 28x28x1\n",
    "xc_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "xc_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "# load labels\n",
    "y_train = df_train.label.values\n",
    "\n",
    "input_size = output_size = x_train.shape[1]\n",
    "input_csize = output_size = xc_train.shape[1]\n",
    "\n",
    "print(input_size, input_csize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true,
    "_uuid": "b63d07ee23dccc150ec2f6b9d49c8e18ddaa7cb4"
   },
   "outputs": [],
   "source": [
    "\"\"\" --------------------------------- Dense Autoencoder model ----------------------------------- \"\"\"\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(256, activation='relu')(input_img)\n",
    "encoded = Dense(EMB_SIZE, activation='relu')(encoded)\n",
    "decoded = Dense(256, activation='sigmoid')(encoded)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adagrad',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true,
    "_uuid": "c2770432b7c0ce2fcc4fb9d6d2ef2c6b8200906b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 42000 samples, validate on 28000 samples\nEpoch 1/100\n42000/42000 [==============================] - 3s 69us/step - loss: 0.3286 - acc: 0.7581 - val_loss: 0.2624 - val_acc: 0.7911\nEpoch 2/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2573 - acc: 0.7903 - val_loss: 0.2516 - val_acc: 0.7925\nEpoch 3/100\n42000/42000 [==============================] - 2s 43us/step - loss: 0.2486 - acc: 0.7902 - val_loss: 0.2452 - val_acc: 0.7890\nEpoch 4/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2434 - acc: 0.7898 - val_loss: 0.2405 - val_acc: 0.7915\nEpoch 5/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.2385 - acc: 0.7902 - val_loss: 0.2355 - val_acc: 0.7899\nEpoch 6/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2340 - acc: 0.7905 - val_loss: 0.2316 - val_acc: 0.7900\nEpoch 7/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2307 - acc: 0.7905 - val_loss: 0.2292 - val_acc: 0.7928\nEpoch 8/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2278 - acc: 0.7909 - val_loss: 0.2225 - val_acc: 0.7949\nEpoch 9/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2172 - acc: 0.7915 - val_loss: 0.2137 - val_acc: 0.7914\nEpoch 10/100\n42000/42000 [==============================] - 2s 43us/step - loss: 0.2127 - acc: 0.7911 - val_loss: 0.2113 - val_acc: 0.7920\nEpoch 11/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2109 - acc: 0.7911 - val_loss: 0.2099 - val_acc: 0.7912\nEpoch 12/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2098 - acc: 0.7911 - val_loss: 0.2089 - val_acc: 0.7918\nEpoch 13/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2089 - acc: 0.7912 - val_loss: 0.2081 - val_acc: 0.7915\nEpoch 14/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.2067 - acc: 0.7920 - val_loss: 0.2039 - val_acc: 0.7869\nEpoch 15/100\n42000/42000 [==============================] - 2s 43us/step - loss: 0.1967 - acc: 0.7947 - val_loss: 0.1922 - val_acc: 0.7956\nEpoch 16/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1908 - acc: 0.7952 - val_loss: 0.1898 - val_acc: 0.7937\nEpoch 17/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.1887 - acc: 0.7954 - val_loss: 0.1879 - val_acc: 0.7947\nEpoch 18/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.1873 - acc: 0.7957 - val_loss: 0.1868 - val_acc: 0.7953\nEpoch 19/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1865 - acc: 0.7958 - val_loss: 0.1858 - val_acc: 0.7966\nEpoch 20/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.1853 - acc: 0.7959 - val_loss: 0.1846 - val_acc: 0.7969\nEpoch 21/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1843 - acc: 0.7961 - val_loss: 0.1838 - val_acc: 0.7972\nEpoch 22/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1834 - acc: 0.7963 - val_loss: 0.1826 - val_acc: 0.7967\nEpoch 23/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1825 - acc: 0.7966 - val_loss: 0.1817 - val_acc: 0.7974\nEpoch 24/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1815 - acc: 0.7971 - val_loss: 0.1805 - val_acc: 0.7988\nEpoch 25/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1793 - acc: 0.7986 - val_loss: 0.1776 - val_acc: 0.7980\nEpoch 26/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.1771 - acc: 0.7993 - val_loss: 0.1766 - val_acc: 0.8015\nEpoch 27/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1761 - acc: 0.7994 - val_loss: 0.1760 - val_acc: 0.8019\nEpoch 28/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1753 - acc: 0.7996 - val_loss: 0.1749 - val_acc: 0.7988\nEpoch 29/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1748 - acc: 0.7996 - val_loss: 0.1745 - val_acc: 0.8012\nEpoch 30/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1745 - acc: 0.7996 - val_loss: 0.1739 - val_acc: 0.8008\nEpoch 31/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1740 - acc: 0.7997 - val_loss: 0.1738 - val_acc: 0.8013\nEpoch 32/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1737 - acc: 0.7997 - val_loss: 0.1732 - val_acc: 0.7997\nEpoch 33/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.1735 - acc: 0.7997 - val_loss: 0.1732 - val_acc: 0.8014\nEpoch 34/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1732 - acc: 0.7998 - val_loss: 0.1728 - val_acc: 0.7994\nEpoch 35/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1731 - acc: 0.7998 - val_loss: 0.1727 - val_acc: 0.8010\nEpoch 36/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1727 - acc: 0.7999 - val_loss: 0.1726 - val_acc: 0.7991\nEpoch 37/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1726 - acc: 0.7998 - val_loss: 0.1724 - val_acc: 0.8012\nEpoch 38/100\n42000/42000 [==============================] - 2s 42us/step - loss: 0.1723 - acc: 0.7999 - val_loss: 0.1720 - val_acc: 0.8006\nEpoch 39/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1721 - acc: 0.7999 - val_loss: 0.1719 - val_acc: 0.8005\nEpoch 40/100\n42000/42000 [==============================] - 2s 41us/step - loss: 0.1720 - acc: 0.7999 - val_loss: 0.1720 - val_acc: 0.7988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba7e72c0b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" --------------------------------- Dense Autoencoder model training ----------------------------------- \"\"\"\n",
    "\n",
    "callbacks=[\n",
    "    EarlyStopping(monitor='val_loss'),\n",
    "]\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                 epochs=100,\n",
    "                 batch_size=1025,\n",
    "                 shuffle=True,\n",
    "                 validation_data=(x_test, x_test),\n",
    "                 callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "0c8117dccaf6d6e33cd027a15e8e515264e842ff"
   },
   "outputs": [],
   "source": [
    "\"\"\" --------------------------------- Convolutional Autoencoder model ----------------------------------- \"\"\"\n",
    "\n",
    "input_img = Input(shape=(input_csize, input_csize, 1))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" --------------------------------- Convolutional Autoencoder model training ----------------------------------- \"\"\"\n",
    "\n",
    "callbacks=[\n",
    "    EarlyStopping(monitor='val_loss'),\n",
    "]\n",
    "\n",
    "autoencoder.fit(xc_train, xc_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(xc_test, xc_test),\n",
    "                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" --------------------------------- Triplet model ----------------------------------- \"\"\"\n",
    "\n",
    "input_img = Input(shape=(input_csize, input_csize, 1))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "embeddings = Lambda(lambda z: K.l2_normalize(z, axis=1))(x)\n",
    "\n",
    "triplet_model = Model(input_img, embeddings)\n",
    "triplet_model.compile(optimizer='adadelta', loss=triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n42000/42000 [==============================] - 3s 73us/step - loss: 0.0000e+00\nEpoch 2/50\n42000/42000 [==============================] - 2s 52us/step - loss: 0.0000e+00\nEpoch 3/50\n42000/42000 [==============================] - 2s 52us/step - loss: 0.0000e+00\nEpoch 4/50\n30720/42000 [====================>.........] - ETA: 0s - loss: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-026f125db280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    119\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3495\u001b[0m     \"\"\"\n\u001b[1;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3497\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msz\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3549\u001b[0m         \u001b[0;31m# warn and return nans like mean would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3550\u001b[0;31m         \u001b[0mrout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_median_nancheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3552\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3118\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\"\"\" --------------------------------- Triplet model training ----------------------------------- \"\"\"\n",
    "\n",
    "callbacks=[\n",
    "    EarlyStopping(monitor='val_loss'),\n",
    "]\n",
    "\n",
    "triplet_model.fit(xc_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
