{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numbers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Lambda, Activation, MaxPooling2D, GlobalAveragePooling2D, Conv2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "\n",
    "\n",
    "INPUT_DIR = '../input'\n",
    "\n",
    "EMB_SIZE = 8\n",
    "N_FOLDS = 4\n",
    "N_ITER = 50\n",
    "SEED = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "trusted": true,
    "_uuid": "b2c9dac6a869920ff0f6a2925ce924a333d33a05",
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "\"\"\" --------------------------------- Triplet loss implementation ----------------------------------- \"\"\"\n",
    "\n",
    "\n",
    "def _all_diffs(a, b):\n",
    "    \"\"\" Returns a tensor of all combinations of a - b.\n",
    "    Args:\n",
    "        a (2D tensor): A batch of vectors shaped (B1, F).\n",
    "        b (2D tensor): A batch of vectors shaped (B2, F).\n",
    "    Returns:\n",
    "        The matrix of all pairwise differences between all vectors in `a` and in\n",
    "        `b`, will be of shape (B1, B2).\n",
    "    Note:\n",
    "        For convenience, if either `a` or `b` is a `Distribution` object, its\n",
    "        mean is used.\n",
    "    \"\"\"\n",
    "    return tf.expand_dims(a, axis=1) - tf.expand_dims(b, axis=0)\n",
    "\n",
    "\n",
    "def _cdist(a, b, metric='euclidean'):\n",
    "    \"\"\"Similar to scipy.spatial's _cdist, but symbolic.\n",
    "    The currently supported metrics can be listed as `_cdist.supported_metrics` and are:\n",
    "        - 'euclidean', although with a fudge-factor epsilon.\n",
    "        - 'sqeuclidean', the squared euclidean.\n",
    "        - 'cityblock', the manhattan or L1 distance.\n",
    "    Args:\n",
    "        a (2D tensor): The left-hand side, shaped (B1, F).\n",
    "        b (2D tensor): The right-hand side, shaped (B2, F).\n",
    "        metric (string): Which distance metric to use, see notes.\n",
    "    Returns:\n",
    "        The matrix of all pairwise distances between all vectors in `a` and in\n",
    "        `b`, will be of shape (B1, B2).\n",
    "    Note:\n",
    "        When a square root is taken (such as in the Euclidean case), a small\n",
    "        epsilon is added because the gradient of the square-root at zero is\n",
    "        undefined. Thus, it will never return exact zero in these cases.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"_cdist\"):\n",
    "        diffs = _all_diffs(a, b)\n",
    "        if metric == 'sqeuclidean':\n",
    "            return tf.reduce_sum(tf.square(diffs), axis=-1)\n",
    "        elif metric == 'euclidean':\n",
    "            return tf.sqrt(tf.reduce_sum(tf.square(diffs), axis=-1) + 1e-12)\n",
    "        elif metric == 'cityblock':\n",
    "            return tf.reduce_sum(tf.abs(diffs), axis=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'The following metric is not implemented by `_cdist` yet: {}'.format(metric))\n",
    "\n",
    "\n",
    "_cdist.supported_metrics = [\n",
    "    'euclidean',\n",
    "    'sqeuclidean',\n",
    "    'cityblock',\n",
    "]\n",
    "\n",
    "\n",
    "def _get_at_indices(tensor, indices):\n",
    "    \"\"\" Like `tensor[np.arange(len(tensor)), indices]` in numpy. \"\"\"\n",
    "    counter = tf.range(tf.shape(indices, out_type=indices.dtype)[0])\n",
    "    return tf.gather_nd(tensor, tf.stack((counter, indices), -1))\n",
    "\n",
    "\n",
    "def batch_hard_loss(features, pids, metric='euclidean', margin=0.1):\n",
    "    \"\"\"Computes the batch-hard loss from arxiv.org/abs/1703.07737.\n",
    "    Args:\n",
    "        dists (2D tensor): A square all-to-all distance matrix as given by _cdist.\n",
    "        pids (1D tensor): The identities of the entries in `batch`, shape (B,).\n",
    "            This can be of any type that can be compared, thus also a string.\n",
    "        margin: The value of the margin if a number, alternatively the string\n",
    "            'soft' for using the soft-margin formulation, or `None` for not\n",
    "            using a margin at all.\n",
    "    Returns:\n",
    "        A 1D tensor of shape (B,) containing the loss value for each sample.\n",
    "        :param margin:\n",
    "        :param features:\n",
    "        :param pids:\n",
    "        :param metric:\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"batch_hard_loss\"):\n",
    "\n",
    "        dists = _cdist(features, features, metric=metric)\n",
    "\n",
    "        pids = tf.argmax(pids, axis=1)\n",
    "\n",
    "        exp_dims0 = tf.expand_dims(pids, axis=0)\n",
    "        exp_dims1 = tf.expand_dims(pids, axis=1)\n",
    "\n",
    "        same_identity_mask = tf.equal(exp_dims1, exp_dims0)\n",
    "\n",
    "        negative_mask = tf.logical_not(same_identity_mask)\n",
    "        positive_mask = tf.logical_xor(same_identity_mask,\n",
    "                                       tf.eye(tf.shape(pids)[0], dtype=tf.bool))\n",
    "\n",
    "        furthest_positive = tf.reduce_max(dists*tf.cast(positive_mask, tf.float32), axis=1)\n",
    "        # closest_negative = tf.map_fn(lambda x: tf.reduce_min(tf.boolean_mask(x[0], x[1])),\n",
    "        #                              (dists, negative_mask), tf.float32)\n",
    "        # Another way of achieving the same, though more hacky:\n",
    "        closest_negative = tf.reduce_min(dists + 1e5*tf.cast(same_identity_mask, tf.float32), axis=1)\n",
    "\n",
    "        diff = furthest_positive - closest_negative\n",
    "        if isinstance(margin, numbers.Real):\n",
    "            diff = tf.maximum(diff + margin, 0.0)\n",
    "        elif margin == 'soft':\n",
    "            diff = tf.nn.softplus(diff)\n",
    "        elif margin is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError('The margin {} is not implemented in batch_hard_loss'.format(margin))\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def triplet_loss(labels, features):\n",
    "    # https://github.com/tensorflow/tensorflow/issues/20253\n",
    "    # from tensorflow.contrib.losses import metric_learning\n",
    "    # return metric_learning.triplet_semihard_loss(K.argmax(labels, axis=1), embeddings, margin=0.2)\n",
    "    return tf.reduce_mean(batch_hard_loss(features, labels, margin=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1    ...     pixel781  pixel782  pixel783\n0      1       0       0    ...            0         0         0\n1      0       0       0    ...            0         0         0\n2      1       0       0    ...            0         0         0\n3      4       0       0    ...            0         0         0\n4      0       0       0    ...            0         0         0\n\n[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ------------------------------------ Data loading -------------------------------------- \"\"\"\n",
    "\n",
    "# load dataframes\n",
    "df_train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "trusted": true,
    "_uuid": "e1d0eb51bd4156994731546544569b9257954c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]] [[[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]]\n[[0. 1. 0. ... 0. 0. 0.]\n [1. 0. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 1. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" --------------------------------- Data preprocessing ----------------------------------- \"\"\"\n",
    "\n",
    "# load pixel values, reshape them to 28x28 pixels and rescale from [0, 255] to [0, 1]\n",
    "x_train = df_train.iloc[:,1:].values.astype('float32') / 255.\n",
    "x_test = df_test.values.astype('float32') / 255.\n",
    "\n",
    "# make images 28x28x1\n",
    "xc_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "xc_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "# load labels\n",
    "y_train = to_categorical(df_train.label.values)\n",
    "\n",
    "input_size = output_size = x_train.shape[1]\n",
    "input_csize = output_csize = xc_train.shape[1]\n",
    "\n",
    "print(xc_train[:5], xc_test[:5])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "trusted": true,
    "_uuid": "b63d07ee23dccc150ec2f6b9d49c8e18ddaa7cb4"
   },
   "outputs": [],
   "source": [
    "\"\"\" -------------------------------------------- Models ------------------------------------------------ \"\"\"\n",
    "\n",
    "def base_network(model_type='triplet', input_shape=input_csize):\n",
    "    \"\"\"\n",
    "    Base network to be shared.\n",
    "    \"\"\"\n",
    "    if model_type == 'autoencoder':\n",
    "        pass\n",
    "    elif model_type == 'triplet':\n",
    "        model = Sequential([\n",
    "            Conv2D(128, (7,7), padding='same', input_shape=(input_csize, input_csize, 1,), activation='relu'),\n",
    "            MaxPooling2D((2,2), (2,2), padding='same'),\n",
    "            Conv2D(256, (5,5), padding='same', activation='relu'),\n",
    "            MaxPooling2D((2,2), (2,2), padding='same'),\n",
    "            Flatten(),\n",
    "            Dense(4, name='embeddings'),\n",
    "            Lambda(lambda z: K.l2_normalize(z, axis=1))\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" --------------------------------- Triplet model ----------------------------------- \"\"\"\n",
    "\n",
    "triplet_model = base_network()\n",
    "triplet_model.compile(optimizer='adadelta', loss=triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n42000/42000 [==============================] - 5s 117us/step - loss: 0.2099\nEpoch 2/25\n42000/42000 [==============================] - 4s 106us/step - loss: 0.2004\nEpoch 3/25\n42000/42000 [==============================] - 4s 107us/step - loss: 0.2001\nEpoch 4/25\n42000/42000 [==============================] - 4s 106us/step - loss: 0.2000\nEpoch 5/25\n42000/42000 [==============================] - 4s 106us/step - loss: 0.1999\nEpoch 6/25\n42000/42000 [==============================] - 4s 107us/step - loss: 0.1998\nEpoch 7/25\n42000/42000 [==============================] - 4s 106us/step - loss: 0.1996\nEpoch 8/25\n42000/42000 [==============================] - 4s 107us/step - loss: 0.2004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff20ba5efd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" --------------------------------- Triplet model training ----------------------------------- \"\"\"\n",
    "\n",
    "callbacks=[\n",
    "    EarlyStopping(monitor='loss'),\n",
    "]\n",
    "\n",
    "triplet_model.fit(xc_train, y_train,\n",
    "                  epochs=25,\n",
    "                  batch_size=128,\n",
    "                  shuffle=True,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xf_train = triplet_model.predict(xc_train)\n",
    "xf_test = triplet_model.predict(xc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ----------------------------- Grid params initialization ------------------------------ \"\"\"\n",
    "\n",
    "MODELS = {\n",
    "#     'lr': {\n",
    "#         'model': LogisticRegression,\n",
    "#         'params': {\n",
    "#             'fit_intercept': [True, False],\n",
    "#             'multi_class': ['ovr'],\n",
    "#             'penalty': ['l2'],\n",
    "#             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "#             'tol': [0.01, 0.05, 0.1, 0.5, 1, 5],\n",
    "#             'random_state': [SEED],\n",
    "#         },\n",
    "#         'best_params': {'tol': 0.05, 'solver': 'newton-cg', 'random_state': 32, 'penalty': 'l2', 'multi_class': 'ovr', 'fit_intercept': True},\n",
    "#         'best score': 0.813692480359147,\n",
    "#     },\n",
    "#     'mlp': {\n",
    "#         'model': MLPClassifier,\n",
    "#         'params': {\n",
    "#             'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "#             'solver' : ['lbfgs', 'adam'],\n",
    "#             'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "#             'learning_rate_init': [.01, .05, .1, .2, .5, 1, 2],\n",
    "#             'random_state': [SEED],\n",
    "#         },\n",
    "#         'best_params': {'solver': 'lbfgs', 'random_state': 32, 'learning_rate_init': 2, 'learning_rate': 'adaptive', 'activation': 'identity'},\n",
    "#         'best_score': 0.8092031425364759,\n",
    "#     },\n",
    "    'knn': {\n",
    "        'model': KNeighborsClassifier,\n",
    "        'params': {\n",
    "            'n_neighbors' : range(1, 5),\n",
    "            'weights' : ['uniform', 'distance'],\n",
    "            'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'leaf_size' : range(10, 100, 10),\n",
    "        },\n",
    "        'best_params': {'weights': 'distance', 'n_neighbors': 2, 'leaf_size': 50, 'algorithm': 'auto'},\n",
    "        'best_score': 1.\n",
    "    },\n",
    "#     'lrcv': {\n",
    "#         'model': LogisticRegressionCV,\n",
    "#         'params': {\n",
    "#             'Cs': [1, 2, 4, 8, 16, 32],\n",
    "#             'fit_intercept': [True, False],\n",
    "#             'refit': [True, False],\n",
    "#             'multi_class': ['ovr'],\n",
    "#             'penalty': ['l2'],\n",
    "#             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "#             'tol': [0.01, 0.05, 0.1, 0.5, 1, 5],\n",
    "#             'cv': [cv]\n",
    "#         },\n",
    "#         'best_params': {'tol': 0.05, 'solver': 'newton-cg', 'refit': True, 'penalty': 'l2', 'multi_class': 'ovr', 'fit_intercept': False, 'cv': 4, 'Cs': 2},\n",
    "#         'best_score': 0.8428731762065096\n",
    "#     },\n",
    "#     'dt': {\n",
    "#         'model': DecisionTreeClassifier,\n",
    "#         'params': {\n",
    "#             'criterion': ['gini', 'entropy'],\n",
    "#             'max_depth': range(6, 10),\n",
    "#             'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "#             'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n",
    "#             'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node\n",
    "#         },\n",
    "#         'best_params': {'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 6, 'criterion': 'gini'},\n",
    "#         'best_score': 0.8181818181818182,\n",
    "#     },\n",
    "#     'svc': {\n",
    "#         'model': SVC,\n",
    "#         'params': {\n",
    "#             'C': [0.1, 0.5, 1., 2., 4.],\n",
    "#             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#             'gamma': ['auto', 'scale'],\n",
    "#             'degree': range(5),\n",
    "#             'tol': [0.1, 0.5, 1, 5],\n",
    "#         },\n",
    "#         'best_params': {'tol': 1, 'shrinking': False, 'probability': False, 'kernel': 'rbf', 'gamma': 'scale', 'degree': 4, 'C': 2.0},\n",
    "#         'best_score': 0.8428731762065096\n",
    "#     },\n",
    "#     'rf': {\n",
    "#         'model': RandomForestClassifier,\n",
    "#         'params': {\n",
    "#             'n_estimators': range(10, 251, 20),\n",
    "#             'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "#             'max_depth': range(5, 20),\n",
    "#             'min_samples_split': range(2, 10), # Minimum number of samples required to split a node\n",
    "#             'min_samples_leaf': range(1, 10), # Minimum number of samples required at each leaf node\n",
    "#             'bootstrap': [True, False], # Method of selecting samples for training each tree,\n",
    "#             'random_state': [SEED],\n",
    "#         },\n",
    "#         'best_params': {'random_state': 32, 'n_jobs': -1, 'n_estimators': 70, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 17, 'bootstrap': True},\n",
    "#         'best_score': 0.8417508417508418\n",
    "#     },\n",
    "#     'ada': {\n",
    "#         'model': AdaBoostClassifier,\n",
    "#         'params': {\n",
    "#             'n_estimators': range(10, 251, 20),\n",
    "#             'learning_rate': [.01, .05, .1, .2, .5, 1, 2],\n",
    "#             'algorithm': ['SAMME', 'SAMME.R'],\n",
    "#             'random_state': [SEED],\n",
    "#         },\n",
    "#         'best_params': {'random_state': 32, 'n_estimators': 170, 'learning_rate': 1, 'algorithm': 'SAMME.R'},\n",
    "#         'best_score': 0.8237934904601572\n",
    "#     },\n",
    "#     'et': {\n",
    "#         'model': ExtraTreesClassifier,\n",
    "#         'params': {\n",
    "#             'n_estimators': range(10, 251, 20),\n",
    "#             'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "#             'max_depth': range(5, 20),\n",
    "#             'min_samples_split': range(2, 10), # Minimum number of samples required to split a node\n",
    "#             'min_samples_leaf': range(1, 10), # Minimum number of samples required at each leaf node\n",
    "#             'bootstrap': [True, False], # Method of selecting samples for training each tree,\n",
    "#             'random_state': [SEED],\n",
    "#         },\n",
    "#         'best_params': {'random_state': 32, 'n_jobs': -1, 'n_estimators': 70, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 11, 'bootstrap': True},\n",
    "#         'best_score': 0.8294051627384961\n",
    "#     },\n",
    "#     'gb': {\n",
    "#         'model': GradientBoostingClassifier,\n",
    "#         'params': {\n",
    "#             'n_estimators': range(10, 251, 20),\n",
    "#             'max_depth': range(5, 20),\n",
    "#             'loss': ['deviance', 'exponential'],\n",
    "#             'learning_rate': [.01, .05, .1, .2, .5, 1, 2],                      \n",
    "#             'subsample': [.25, .5, .8, 1.],\n",
    "#             'min_samples_split': range(2, 10), # Minimum number of samples required to split a node\n",
    "#             'min_samples_leaf': range(1, 10), # Minimum number of samples required at each leaf node\n",
    "#             'random_state': [SEED],\n",
    "#         },\n",
    "#         'best_params': {'subsample': 0.5, 'random_state': 32, 'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 13, 'loss': 'exponential', 'learning_rate': 1},\n",
    "#         'best_score': 0.8361391694725028\n",
    "#     }\n",
    "#     'xgb': {\n",
    "#         'model': XGBClassifier,\n",
    "#         'params': {\n",
    "#             'n_estimators': range(8, 20),\n",
    "#             'max_depth': range(5, 20),\n",
    "#             'learning_rate': [.01, .05, .1, .2, .5, 1, 2],\n",
    "#             'colsample_bytree': [.6, .7, .8, .9, 1]\n",
    "#         }\n",
    "#     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting knn...\nAccuracy of model knn: 0.9644285714285714 (SMALLER for 0.03557142857142859)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ---------------------------- Best models configuration search --------------------------- \"\"\"\n",
    "\n",
    "FIT_FROM_SCRATCH = False\n",
    "\n",
    "for name, model in MODELS.items():\n",
    "    \n",
    "    if 'best_score' in model and not FIT_FROM_SCRATCH:\n",
    "        \n",
    "        # Initialize with best parameters & fit to data\n",
    "        print(f'Fitting {name}...')\n",
    "        \n",
    "        model['best_estimator'] = model['model'](**model['best_params']).fit(xf_train, y_train)\n",
    "        \n",
    "        scores = cross_val_score(model['best_estimator'], xf_train, y_train, cv=N_FOLDS)\n",
    "        score = sum(scores) / len(scores)\n",
    "        diff = score - model['best_score']\n",
    "        \n",
    "        if diff > 0:\n",
    "            print(f'Accuracy of model {name}: {score} (BIGGER for {diff})')\n",
    "        elif diff < 0:\n",
    "            print(f'Accuracy of model {name}: {score} (SMALLER for {-diff})')\n",
    "        else:\n",
    "            print(f'Accuracy of model {name}: {score} (SAME)')\n",
    "    else:\n",
    "        # Perform random search\n",
    "        searcher = RandomizedSearchCV(param_distributions=model['params'],\n",
    "                                      estimator=model['model'](), scoring=\"accuracy\",\n",
    "                                      verbose=1, n_iter=N_ITER, cv=N_FOLDS)\n",
    "        # Fit to data\n",
    "        print(f'Fitting {name}...')\n",
    "        \n",
    "        searcher.fit(xf_train, y_train)\n",
    "\n",
    "        # Print the best parameters and best accuracy\n",
    "        print(f'Best parameters found for {name}: {searcher.best_params_}')\n",
    "        print(f'Best accuracy found {name}: {searcher.best_score_}')\n",
    "\n",
    "        model['best_estimator'] = searcher.best_estimator_\n",
    "        model['best_params'] = searcher.best_params_\n",
    "        model['best_score'] = searcher.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n"
     ]
    }
   ],
   "source": [
    "pred = MODELS['knn']['best_estimator'].predict(xf_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'ImageId': range(1, pred.shape[0]+1), 'Label': pred})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
