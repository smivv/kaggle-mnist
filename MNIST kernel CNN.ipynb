{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numbers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Lambda, Activation, MaxPooling2D, \\\n",
    "                            GlobalAveragePooling2D, Conv2D, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "\n",
    "INPUT_DIR = '../input'\n",
    "EMB_SIZE = 8\n",
    "BATCH_SIZE = 1024\n",
    "N_FOLDS = 2\n",
    "N_ITER = 50\n",
    "SEED = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true,
    "_uuid": "b2c9dac6a869920ff0f6a2925ce924a333d33a05",
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "\"\"\" --------------------------------- Triplet loss implementation ----------------------------------- \"\"\"\n",
    "\n",
    "\n",
    "def _all_diffs(a, b):\n",
    "    \"\"\" Returns a tensor of all combinations of a - b.\n",
    "    Args:\n",
    "        a (2D tensor): A batch of vectors shaped (B1, F).\n",
    "        b (2D tensor): A batch of vectors shaped (B2, F).\n",
    "    Returns:\n",
    "        The matrix of all pairwise differences between all vectors in `a` and in\n",
    "        `b`, will be of shape (B1, B2).\n",
    "    Note:\n",
    "        For convenience, if either `a` or `b` is a `Distribution` object, its\n",
    "        mean is used.\n",
    "    \"\"\"\n",
    "    return tf.expand_dims(a, axis=1) - tf.expand_dims(b, axis=0)\n",
    "\n",
    "\n",
    "def _cdist(a, b, metric='euclidean'):\n",
    "    \"\"\"Similar to scipy.spatial's _cdist, but symbolic.\n",
    "    The currently supported metrics can be listed as `_cdist.supported_metrics` and are:\n",
    "        - 'euclidean', although with a fudge-factor epsilon.\n",
    "        - 'sqeuclidean', the squared euclidean.\n",
    "        - 'cityblock', the manhattan or L1 distance.\n",
    "    Args:\n",
    "        a (2D tensor): The left-hand side, shaped (B1, F).\n",
    "        b (2D tensor): The right-hand side, shaped (B2, F).\n",
    "        metric (string): Which distance metric to use, see notes.\n",
    "    Returns:\n",
    "        The matrix of all pairwise distances between all vectors in `a` and in\n",
    "        `b`, will be of shape (B1, B2).\n",
    "    Note:\n",
    "        When a square root is taken (such as in the Euclidean case), a small\n",
    "        epsilon is added because the gradient of the square-root at zero is\n",
    "        undefined. Thus, it will never return exact zero in these cases.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"_cdist\"):\n",
    "        diffs = _all_diffs(a, b)\n",
    "        if metric == 'sqeuclidean':\n",
    "            return tf.reduce_sum(tf.square(diffs), axis=-1)\n",
    "        elif metric == 'euclidean':\n",
    "            return tf.sqrt(tf.reduce_sum(tf.square(diffs), axis=-1) + 1e-12)\n",
    "        elif metric == 'cityblock':\n",
    "            return tf.reduce_sum(tf.abs(diffs), axis=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'The following metric is not implemented by `_cdist` yet: {}'.format(metric))\n",
    "\n",
    "\n",
    "_cdist.supported_metrics = [\n",
    "    'euclidean',\n",
    "    'sqeuclidean',\n",
    "    'cityblock',\n",
    "]\n",
    "\n",
    "\n",
    "def _get_at_indices(tensor, indices):\n",
    "    \"\"\" Like `tensor[np.arange(len(tensor)), indices]` in numpy. \"\"\"\n",
    "    counter = tf.range(tf.shape(indices, out_type=indices.dtype)[0])\n",
    "    return tf.gather_nd(tensor, tf.stack((counter, indices), -1))\n",
    "\n",
    "\n",
    "def batch_hard_loss(features, pids, metric='euclidean', margin=0.1):\n",
    "    \"\"\"Computes the batch-hard loss from arxiv.org/abs/1703.07737.\n",
    "    Args:\n",
    "        dists (2D tensor): A square all-to-all distance matrix as given by _cdist.\n",
    "        pids (1D tensor): The identities of the entries in `batch`, shape (B,).\n",
    "            This can be of any type that can be compared, thus also a string.\n",
    "        margin: The value of the margin if a number, alternatively the string\n",
    "            'soft' for using the soft-margin formulation, or `None` for not\n",
    "            using a margin at all.\n",
    "    Returns:\n",
    "        A 1D tensor of shape (B,) containing the loss value for each sample.\n",
    "        :param margin:\n",
    "        :param features:\n",
    "        :param pids:\n",
    "        :param metric:\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"batch_hard_loss\"):\n",
    "\n",
    "        dists = _cdist(features, features, metric=metric)\n",
    "\n",
    "        pids = tf.argmax(pids, axis=1)\n",
    "\n",
    "        exp_dims0 = tf.expand_dims(pids, axis=0)\n",
    "        exp_dims1 = tf.expand_dims(pids, axis=1)\n",
    "\n",
    "        same_identity_mask = tf.equal(exp_dims1, exp_dims0)\n",
    "\n",
    "        negative_mask = tf.logical_not(same_identity_mask)\n",
    "        positive_mask = tf.logical_xor(same_identity_mask,\n",
    "                                       tf.eye(tf.shape(pids)[0], dtype=tf.bool))\n",
    "\n",
    "        furthest_positive = tf.reduce_max(dists*tf.cast(positive_mask, tf.float32), axis=1)\n",
    "        # closest_negative = tf.map_fn(lambda x: tf.reduce_min(tf.boolean_mask(x[0], x[1])),\n",
    "        #                              (dists, negative_mask), tf.float32)\n",
    "        # Another way of achieving the same, though more hacky:\n",
    "        closest_negative = tf.reduce_min(dists + 1e5*tf.cast(same_identity_mask, tf.float32), axis=1)\n",
    "\n",
    "        diff = furthest_positive - closest_negative\n",
    "        if isinstance(margin, numbers.Real):\n",
    "            diff = tf.maximum(diff + margin, 0.0)\n",
    "        elif margin == 'soft':\n",
    "            diff = tf.nn.softplus(diff)\n",
    "        elif margin is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError('The margin {} is not implemented in batch_hard_loss'.format(margin))\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def triplet_loss(labels, features):\n",
    "    # https://github.com/tensorflow/tensorflow/issues/20253\n",
    "    # from tensorflow.contrib.losses import metric_learning\n",
    "    # return metric_learning.triplet_semihard_loss(K.argmax(labels, axis=1), embeddings, margin=0.2)\n",
    "    return tf.reduce_mean(batch_hard_loss(features, labels, margin=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    return 2 * ((precision * recall)/(precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1    ...     pixel781  pixel782  pixel783\n0      1       0       0    ...            0         0         0\n1      0       0       0    ...            0         0         0\n2      1       0       0    ...            0         0         0\n3      4       0       0    ...            0         0         0\n4      0       0       0    ...            0         0         0\n\n[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ------------------------------------ Data loading -------------------------------------- \"\"\"\n",
    "\n",
    "# load dataframes\n",
    "df_train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "trusted": true,
    "_uuid": "e1d0eb51bd4156994731546544569b9257954c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]] [[[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" --------------------------------- Data preprocessing ----------------------------------- \"\"\"\n",
    "\n",
    "# load pixel values, reshape them to 28x28 pixels and rescale from [0, 255] to [0, 1]\n",
    "x_train = df_train.iloc[:,1:].values.astype('float32') / 255.\n",
    "x_test = df_test.values.astype('float32') / 255.\n",
    "\n",
    "# make images 28x28x1\n",
    "xc_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "xc_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "# load labels\n",
    "y_train = df_train.label.values\n",
    "yc_train = to_categorical(y_train)\n",
    "\n",
    "input_size = output_size = x_train.shape[1]\n",
    "input_csize = output_csize = xc_train.shape[1]\n",
    "\n",
    "print(xc_train[:5], xc_test[:5])\n",
    "# print(yc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "trusted": true,
    "_uuid": "b63d07ee23dccc150ec2f6b9d49c8e18ddaa7cb4"
   },
   "outputs": [],
   "source": [
    "\"\"\" -------------------------------------------- Models ------------------------------------------------ \"\"\"\n",
    "\n",
    "def base_network(model_type='triplet', input_shape=input_csize):\n",
    "    \"\"\"\n",
    "    Base network to be shared.\n",
    "    \"\"\"\n",
    "    if model_type == 'autoencoder':\n",
    "        pass\n",
    "    elif model_type == 'triplet':\n",
    "        model = Sequential([\n",
    "            Conv2D(filters=64, kernel_size=(3, 3), padding='same', input_shape=(input_csize, input_csize, 1,), activation='relu'),\n",
    "            Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "            Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "            Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.25),\n",
    "            Dense(10, name='embeddings', activation='softmax'),\n",
    "#             Lambda(lambda z: K.l2_normalize(z, axis=1))\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" --------------------------------- Data generator ----------------------------------- \"\"\"\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False, # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(xc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 42000\nEpoch 1/50\n - 13s - loss: 0.8015 - f1: 0.7426 - val_loss: 0.9149 - val_f1: 0.7334\nEpoch 2/50\n - 8s - loss: 0.1938 - f1: 0.9390 - val_loss: 0.2930 - val_f1: 0.9238\nEpoch 3/50\n - 11s - loss: 0.1427 - f1: 0.9566 - val_loss: 0.1615 - val_f1: 0.9581\nEpoch 4/50\n - 11s - loss: 0.1050 - f1: 0.9669 - val_loss: 0.1483 - val_f1: 0.9644\n\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 5/50\n - 11s - loss: 0.0598 - f1: 0.9816 - val_loss: 0.0701 - val_f1: 0.9783\nEpoch 6/50\n - 11s - loss: 0.0496 - f1: 0.9844 - val_loss: 0.0745 - val_f1: 0.9776\nEpoch 7/50\n - 11s - loss: 0.0452 - f1: 0.9866 - val_loss: 0.0616 - val_f1: 0.9818\n\nEpoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 8/50\n - 11s - loss: 0.0369 - f1: 0.9882 - val_loss: 0.0419 - val_f1: 0.9879\nEpoch 9/50\n - 12s - loss: 0.0302 - f1: 0.9906 - val_loss: 0.0331 - val_f1: 0.9904\nEpoch 10/50\n - 12s - loss: 0.0273 - f1: 0.9913 - val_loss: 0.0459 - val_f1: 0.9862\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 11/50\n - 11s - loss: 0.0240 - f1: 0.9925 - val_loss: 0.0369 - val_f1: 0.9890\nEpoch 12/50\n - 13s - loss: 0.0226 - f1: 0.9928 - val_loss: 0.0301 - val_f1: 0.9911\nEpoch 13/50\n - 11s - loss: 0.0224 - f1: 0.9931 - val_loss: 0.0310 - val_f1: 0.9911\n\nEpoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 14/50\n - 11s - loss: 0.0200 - f1: 0.9938 - val_loss: 0.0253 - val_f1: 0.9918\nEpoch 15/50\n - 11s - loss: 0.0203 - f1: 0.9932 - val_loss: 0.0269 - val_f1: 0.9926\nEpoch 16/50\n - 11s - loss: 0.0191 - f1: 0.9940 - val_loss: 0.0270 - val_f1: 0.9923\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\nEpoch 17/50\n - 11s - loss: 0.0172 - f1: 0.9945 - val_loss: 0.0269 - val_f1: 0.9923\nEpoch 18/50\n - 11s - loss: 0.0175 - f1: 0.9944 - val_loss: 0.0228 - val_f1: 0.9929\nEpoch 19/50\n - 13s - loss: 0.0154 - f1: 0.9950 - val_loss: 0.0252 - val_f1: 0.9929\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\nEpoch 20/50\n - 11s - loss: 0.0166 - f1: 0.9949 - val_loss: 0.0241 - val_f1: 0.9933\nEpoch 21/50\n - 11s - loss: 0.0173 - f1: 0.9942 - val_loss: 0.0252 - val_f1: 0.9932\nEpoch 22/50\n - 11s - loss: 0.0165 - f1: 0.9952 - val_loss: 0.0243 - val_f1: 0.9929\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 1e-05.\nEpoch 23/50\n - 11s - loss: 0.0165 - f1: 0.9953 - val_loss: 0.0222 - val_f1: 0.9936\nEpoch 24/50\n - 11s - loss: 0.0156 - f1: 0.9952 - val_loss: 0.0252 - val_f1: 0.9927\nEpoch 25/50\n - 11s - loss: 0.0155 - f1: 0.9953 - val_loss: 0.0218 - val_f1: 0.9941\nEpoch 26/50\n - 13s - loss: 0.0152 - f1: 0.9949 - val_loss: 0.0274 - val_f1: 0.9931\nEpoch 27/50\n - 11s - loss: 0.0155 - f1: 0.9953 - val_loss: 0.0223 - val_f1: 0.9937\nEpoch 28/50\n - 11s - loss: 0.0140 - f1: 0.9955 - val_loss: 0.0215 - val_f1: 0.9942\nEpoch 29/50\n - 11s - loss: 0.0163 - f1: 0.9952 - val_loss: 0.0251 - val_f1: 0.9933\nEpoch 30/50\n - 11s - loss: 0.0162 - f1: 0.9947 - val_loss: 0.0238 - val_f1: 0.9936\nEpoch 31/50\n - 11s - loss: 0.0145 - f1: 0.9957 - val_loss: 0.0229 - val_f1: 0.9936\nEpoch 32/50\n - 11s - loss: 0.0152 - f1: 0.9956 - val_loss: 0.0222 - val_f1: 0.9931\nEpoch 33/50\n - 12s - loss: 0.0159 - f1: 0.9947 - val_loss: 0.0231 - val_f1: 0.9934\nEpoch 34/50\n - 11s - loss: 0.0159 - f1: 0.9952 - val_loss: 0.0243 - val_f1: 0.9932\nEpoch 35/50\n - 11s - loss: 0.0163 - f1: 0.9951 - val_loss: 0.0243 - val_f1: 0.9929\nEpoch 36/50\n - 12s - loss: 0.0164 - f1: 0.9944 - val_loss: 0.0229 - val_f1: 0.9938\nEpoch 37/50\n - 12s - loss: 0.0148 - f1: 0.9955 - val_loss: 0.0246 - val_f1: 0.9931\nEpoch 38/50\n - 11s - loss: 0.0163 - f1: 0.9949 - val_loss: 0.0221 - val_f1: 0.9935\nEpoch 39/50\n - 11s - loss: 0.0144 - f1: 0.9959 - val_loss: 0.0256 - val_f1: 0.9931\nEpoch 40/50\n - 12s - loss: 0.0135 - f1: 0.9959 - val_loss: 0.0213 - val_f1: 0.9941\nEpoch 41/50\n - 11s - loss: 0.0150 - f1: 0.9953 - val_loss: 0.0235 - val_f1: 0.9938\nEpoch 42/50\n - 11s - loss: 0.0149 - f1: 0.9953 - val_loss: 0.0229 - val_f1: 0.9936\nEpoch 43/50\n - 11s - loss: 0.0141 - f1: 0.9956 - val_loss: 0.0228 - val_f1: 0.9937\nEpoch 44/50\n - 11s - loss: 0.0140 - f1: 0.9955 - val_loss: 0.0244 - val_f1: 0.9931\nEpoch 45/50\n - 11s - loss: 0.0142 - f1: 0.9953 - val_loss: 0.0222 - val_f1: 0.9938\nEpoch 46/50\n - 11s - loss: 0.0155 - f1: 0.9949 - val_loss: 0.0229 - val_f1: 0.9937\nEpoch 47/50\n - 12s - loss: 0.0152 - f1: 0.9953 - val_loss: 0.0242 - val_f1: 0.9936\nEpoch 48/50\n - 12s - loss: 0.0159 - f1: 0.9952 - val_loss: 0.0228 - val_f1: 0.9936\nEpoch 49/50\n - 11s - loss: 0.0136 - f1: 0.9957 - val_loss: 0.0211 - val_f1: 0.9938\nEpoch 50/50\n - 11s - loss: 0.0121 - f1: 0.9963 - val_loss: 0.0235 - val_f1: 0.9936\n0.9957421048693723\nEpoch 1/50\n - 13s - loss: 0.8419 - f1: 0.7292 - val_loss: 0.2855 - val_f1: 0.9133\nEpoch 2/50\n - 7s - loss: 0.1694 - f1: 0.9478 - val_loss: 0.1753 - val_f1: 0.9479\nEpoch 3/50\n - 11s - loss: 0.1332 - f1: 0.9585 - val_loss: 0.3381 - val_f1: 0.9185\nEpoch 4/50\n - 12s - loss: 0.0914 - f1: 0.9725 - val_loss: 0.1762 - val_f1: 0.9502\n\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 5/50\n - 9s - loss: 0.0564 - f1: 0.9838 - val_loss: 0.1231 - val_f1: 0.9638\nEpoch 6/50\n - 11s - loss: 0.0507 - f1: 0.9844 - val_loss: 0.1166 - val_f1: 0.9669\nEpoch 7/50\n - 11s - loss: 0.0414 - f1: 0.9875 - val_loss: 0.0979 - val_f1: 0.9730\n\nEpoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 8/50\n - 11s - loss: 0.0336 - f1: 0.9900 - val_loss: 0.0400 - val_f1: 0.9875\nEpoch 9/50\n - 11s - loss: 0.0311 - f1: 0.9906 - val_loss: 0.0329 - val_f1: 0.9893\nEpoch 10/50\n - 11s - loss: 0.0258 - f1: 0.9923 - val_loss: 0.0306 - val_f1: 0.9910\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 11/50\n - 13s - loss: 0.0269 - f1: 0.9913 - val_loss: 0.0346 - val_f1: 0.9894\nEpoch 12/50\n - 11s - loss: 0.0217 - f1: 0.9935 - val_loss: 0.0276 - val_f1: 0.9918\nEpoch 13/50\n - 13s - loss: 0.0243 - f1: 0.9926 - val_loss: 0.0305 - val_f1: 0.9905\n\nEpoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 14/50\n - 11s - loss: 0.0200 - f1: 0.9942 - val_loss: 0.0261 - val_f1: 0.9918\nEpoch 15/50\n - 11s - loss: 0.0202 - f1: 0.9943 - val_loss: 0.0207 - val_f1: 0.9936\nEpoch 16/50\n - 11s - loss: 0.0191 - f1: 0.9944 - val_loss: 0.0242 - val_f1: 0.9924\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\nEpoch 17/50\n - 11s - loss: 0.0191 - f1: 0.9943 - val_loss: 0.0239 - val_f1: 0.9926\nEpoch 18/50\n - 13s - loss: 0.0169 - f1: 0.9949 - val_loss: 0.0233 - val_f1: 0.9930\nEpoch 19/50\n - 11s - loss: 0.0175 - f1: 0.9950 - val_loss: 0.0214 - val_f1: 0.9931\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\nEpoch 20/50\n - 11s - loss: 0.0172 - f1: 0.9945 - val_loss: 0.0224 - val_f1: 0.9934\nEpoch 21/50\n - 11s - loss: 0.0181 - f1: 0.9948 - val_loss: 0.0251 - val_f1: 0.9927\nEpoch 22/50\n - 11s - loss: 0.0175 - f1: 0.9942 - val_loss: 0.0235 - val_f1: 0.9931\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 1e-05.\nEpoch 23/50\n - 11s - loss: 0.0165 - f1: 0.9950 - val_loss: 0.0229 - val_f1: 0.9932\nEpoch 24/50\n - 11s - loss: 0.0170 - f1: 0.9952 - val_loss: 0.0218 - val_f1: 0.9934\nEpoch 25/50\n - 13s - loss: 0.0183 - f1: 0.9944 - val_loss: 0.0229 - val_f1: 0.9933\nEpoch 26/50\n - 11s - loss: 0.0144 - f1: 0.9958 - val_loss: 0.0228 - val_f1: 0.9933\nEpoch 27/50\n - 11s - loss: 0.0175 - f1: 0.9947 - val_loss: 0.0211 - val_f1: 0.9932\nEpoch 28/50\n - 11s - loss: 0.0168 - f1: 0.9949 - val_loss: 0.0243 - val_f1: 0.9928\nEpoch 29/50\n - 11s - loss: 0.0162 - f1: 0.9953 - val_loss: 0.0245 - val_f1: 0.9922\nEpoch 30/50\n - 11s - loss: 0.0150 - f1: 0.9958 - val_loss: 0.0221 - val_f1: 0.9933\nEpoch 31/50\n - 11s - loss: 0.0155 - f1: 0.9955 - val_loss: 0.0206 - val_f1: 0.9932\nEpoch 32/50\n - 12s - loss: 0.0181 - f1: 0.9944 - val_loss: 0.0195 - val_f1: 0.9942\nEpoch 33/50\n - 11s - loss: 0.0171 - f1: 0.9946 - val_loss: 0.0238 - val_f1: 0.9928\nEpoch 34/50\n - 11s - loss: 0.0139 - f1: 0.9957 - val_loss: 0.0195 - val_f1: 0.9939\nEpoch 35/50\n - 11s - loss: 0.0164 - f1: 0.9956 - val_loss: 0.0229 - val_f1: 0.9931\nEpoch 36/50\n - 11s - loss: 0.0151 - f1: 0.9960 - val_loss: 0.0197 - val_f1: 0.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n - 11s - loss: 0.0144 - f1: 0.9957 - val_loss: 0.0213 - val_f1: 0.9939\nEpoch 38/50\n - 11s - loss: 0.0160 - f1: 0.9952 - val_loss: 0.0216 - val_f1: 0.9931\nEpoch 39/50\n - 12s - loss: 0.0168 - f1: 0.9954 - val_loss: 0.0223 - val_f1: 0.9933\nEpoch 40/50\n - 13s - loss: 0.0157 - f1: 0.9957 - val_loss: 0.0224 - val_f1: 0.9934\nEpoch 41/50\n - 12s - loss: 0.0141 - f1: 0.9956 - val_loss: 0.0203 - val_f1: 0.9938\nEpoch 42/50\n - 11s - loss: 0.0158 - f1: 0.9951 - val_loss: 0.0204 - val_f1: 0.9932\nEpoch 43/50\n - 11s - loss: 0.0147 - f1: 0.9955 - val_loss: 0.0215 - val_f1: 0.9938\nEpoch 44/50\n - 11s - loss: 0.0137 - f1: 0.9958 - val_loss: 0.0226 - val_f1: 0.9932\nEpoch 45/50\n - 11s - loss: 0.0153 - f1: 0.9947 - val_loss: 0.0215 - val_f1: 0.9941\nEpoch 46/50\n - 12s - loss: 0.0160 - f1: 0.9950 - val_loss: 0.0198 - val_f1: 0.9935\nEpoch 47/50\n - 11s - loss: 0.0148 - f1: 0.9957 - val_loss: 0.0236 - val_f1: 0.9929\nEpoch 48/50\n - 11s - loss: 0.0151 - f1: 0.9954 - val_loss: 0.0206 - val_f1: 0.9936\nEpoch 49/50\n - 11s - loss: 0.0141 - f1: 0.9959 - val_loss: 0.0221 - val_f1: 0.9935\nEpoch 50/50\n - 11s - loss: 0.0137 - f1: 0.9964 - val_loss: 0.0216 - val_f1: 0.9935\n0.9958395417032364\n"
     ]
    }
   ],
   "source": [
    "\"\"\" --------------------------------- Triplet model training ----------------------------------- \"\"\"\n",
    "\n",
    "yfull_test = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, random_state=SEED, shuffle=True)\n",
    "\n",
    "print(len(xc_train), len(y_train))\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(xc_train, y_train)):\n",
    "\n",
    "    triplet_model = base_network()\n",
    "    triplet_model.compile(optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), \n",
    "                          loss=['categorical_crossentropy'], metrics=[f1])\n",
    "#     triplet_model.summary()\n",
    "    \n",
    "    weights_path = os.path.join('.', f'w{i}.h5')\n",
    "\n",
    "    callbacks=[\n",
    "    #         EarlyStopping(monitor='val_f1', min_delta=.0001),\n",
    "        ReduceLROnPlateau(monitor='val_f1', patience=3, verbose=1, factor=0.5, min_lr=0.00001),\n",
    "        ModelCheckpoint(weights_path, monitor='val_f1', mode='max', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    xb_train = xc_train[train_index]\n",
    "    yb_train = yc_train[train_index]\n",
    "    xb_val = xc_train[val_index]\n",
    "    yb_val = yc_train[val_index]\n",
    "\n",
    "    history = triplet_model.fit_generator(\n",
    "        datagen.flow(xb_train, yb_train, batch_size=BATCH_SIZE),\n",
    "        validation_data=datagen.flow(xb_val, yb_val, batch_size=BATCH_SIZE),\n",
    "        validation_steps=xb_val.shape[0] // BATCH_SIZE,\n",
    "        verbose=2,\n",
    "        epochs=N_ITER,\n",
    "        steps_per_epoch=xb_train.shape[0] // BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks)\n",
    "        \n",
    "    if os.path.isfile(weights_path):\n",
    "        triplet_model.load_weights(weights_path)\n",
    "\n",
    "    print(fbeta_score(yb_val, np.array(triplet_model.predict(xb_val, batch_size=128, verbose=2)) > 0.2, beta=2, average='samples'))\n",
    "\n",
    "    yfull_test.append(triplet_model.predict(xc_test, batch_size=128, verbose=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# \"\"\" --------------------------------- Triplet model training ----------------------------------- \"\"\"\n",
    "\n",
    "# yfull_test = []\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=N_FOLDS, random_state=SEED, shuffle=True)\n",
    "\n",
    "# print(len(xc_train), len(y_train))\n",
    "\n",
    "# for i, (train_index, val_index) in enumerate(skf.split(xc_train, yc_train)):\n",
    "\n",
    "#     triplet_model = base_network()\n",
    "#     triplet_model.compile(optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), \n",
    "#                           loss=['categorical_crossentropy'], metrics=[f1])\n",
    "# #     triplet_model.summary()\n",
    "    \n",
    "#     weights_path = os.path.join('.', f'w{i}.h5')\n",
    "\n",
    "#     callbacks=[\n",
    "# #         EarlyStopping(monitor='val_f1', min_delta=.0001),\n",
    "#         ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001),\n",
    "#         ModelCheckpoint(weights_path, monitor='val_f1', mode='max', save_best_only=True, verbose=0)\n",
    "#     ]\n",
    "    \n",
    "#     xb_train = xc_train[train_index]\n",
    "#     yb_train = yc_train[train_index]\n",
    "#     xb_val = xc_train[val_index]\n",
    "#     yb_val = yc_train[val_index]\n",
    "\n",
    "#     triplet_model.fit(\n",
    "# #         xb_train, \n",
    "# #         yb_train,\n",
    "#         datagen.flow(xc_train, yc_train, batch_size=BATCH_SIZE),\n",
    "#         validation_data=(xb_val, yb_val),\n",
    "#         verbose=2,\n",
    "#         epochs=25,\n",
    "#         batch_size=128,\n",
    "#         shuffle=True,\n",
    "#         callbacks=callbacks)\n",
    "        \n",
    "#     if os.path.isfile(weights_path):\n",
    "#         triplet_model.load_weights(weights_path)\n",
    "\n",
    "#     y_pred = triplet_model.predict(xb_val, batch_size=128, verbose=2)\n",
    "#     print(fbeta_score(yb_val, np.array(y_pred) > 0.2, beta=2, average='samples'))\n",
    "\n",
    "#     yfull_test.append(triplet_model.predict(xc_test, batch_size=128, verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000,)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array(yfull_test)\n",
    "pred = np.argmax(pred, axis=2)\n",
    "values, counts = np.unique(pred, axis=0, return_counts=True)\n",
    "pred = values[np.argmax(counts)]\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# \"\"\" ----------------------------- Grid params initialization ------------------------------ \"\"\"\n",
    "\n",
    "# MODELS = {\n",
    "# #     'lr': {\n",
    "# #         'model': LogisticRegression,\n",
    "# #         'params': {\n",
    "# #             'fit_intercept': [True, False],\n",
    "# #             'multi_class': ['ovr'],\n",
    "# #             'penalty': ['l2'],\n",
    "# #             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "# #             'tol': [0.01, 0.05, 0.1, 0.5, 1, 5],\n",
    "# #             'random_state': [SEED],\n",
    "# #         },\n",
    "# #         'best_params': {'tol': 0.05, 'solver': 'newton-cg', 'random_state': 32, 'penalty': 'l2', 'multi_class': 'ovr', 'fit_intercept': True},\n",
    "# #         'best score': 0.813692480359147,\n",
    "# #     },\n",
    "# #     'mlp': {\n",
    "# #         'model': MLPClassifier,\n",
    "# #         'params': {\n",
    "# #             'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "# #             'solver' : ['lbfgs', 'adam'],\n",
    "# #             'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "# #             'learning_rate_init': [.01, .05, .1, .2, .5, 1, 2],\n",
    "# #             'random_state': [SEED],\n",
    "# #         },\n",
    "# #         'best_params': {'solver': 'lbfgs', 'random_state': 32, 'learning_rate_init': 2, 'learning_rate': 'adaptive', 'activation': 'identity'},\n",
    "# #         'best_score': 0.8092031425364759,\n",
    "# #     },\n",
    "#     'knn': {\n",
    "#         'model': KNeighborsClassifier,\n",
    "#         'params': {\n",
    "#             'n_neighbors' : range(1, 5),\n",
    "#             'weights' : ['uniform', 'distance'],\n",
    "#             'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "#             'leaf_size' : range(10, 100, 10),\n",
    "#         },\n",
    "#         'best_params': {'weights': 'distance', 'n_neighbors': 2, 'leaf_size': 50, 'algorithm': 'auto'},\n",
    "#         'best_score': 1.\n",
    "#     },\n",
    "# #     'lrcv': {\n",
    "# #         'model': LogisticRegressionCV,\n",
    "# #         'params': {\n",
    "# #             'Cs': [1, 2, 4, 8, 16, 32],\n",
    "# #             'fit_intercept': [True, False],\n",
    "# #             'refit': [True, False],\n",
    "# #             'multi_class': ['ovr'],\n",
    "# #             'penalty': ['l2'],\n",
    "# #             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "# #             'tol': [0.01, 0.05, 0.1, 0.5, 1, 5],\n",
    "# #             'cv': [cv]\n",
    "# #         },\n",
    "# #         'best_params': {'tol': 0.05, 'solver': 'newton-cg', 'refit': True, 'penalty': 'l2', 'multi_class': 'ovr', 'fit_intercept': False, 'cv': 4, 'Cs': 2},\n",
    "# #         'best_score': 0.8428731762065096\n",
    "# #     },\n",
    "# #     'dt': {\n",
    "# #         'model': DecisionTreeClassifier,\n",
    "# #         'params': {\n",
    "# #             'criterion': ['gini', 'entropy'],\n",
    "# #             'max_depth': range(6, 10),\n",
    "# #             'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "# #             'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n",
    "# #             'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node\n",
    "# #         },\n",
    "# #         'best_params': {'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 6, 'criterion': 'gini'},\n",
    "# #         'best_score': 0.8181818181818182,\n",
    "# #     },\n",
    "# #     'svc': {\n",
    "# #         'model': SVC,\n",
    "# #         'params': {\n",
    "# #             'C': [0.1, 0.5, 1., 2., 4.],\n",
    "# #             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "# #             'gamma': ['auto', 'scale'],\n",
    "# #             'degree': range(5),\n",
    "# #             'tol': [0.1, 0.5, 1, 5],\n",
    "# #         },\n",
    "# #         'best_params': {'tol': 1, 'shrinking': False, 'probability': False, 'kernel': 'rbf', 'gamma': 'scale', 'degree': 4, 'C': 2.0},\n",
    "# #         'best_score': 0.8428731762065096\n",
    "# #     },\n",
    "# #     'rf': {\n",
    "# #         'model': RandomForestClassifier,\n",
    "# #         'params': {\n",
    "# #             'n_estimators': range(10, 251, 20),\n",
    "# #             'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "# #             'max_depth': range(5, 20),\n",
    "# #             'min_samples_split': range(2, 10), # Minimum number of samples required to split a node\n",
    "# #             'min_samples_leaf': range(1, 10), # Minimum number of samples required at each leaf node\n",
    "# #             'bootstrap': [True, False], # Method of selecting samples for training each tree,\n",
    "# #             'random_state': [SEED],\n",
    "# #         },\n",
    "# #         'best_params': {'random_state': 32, 'n_jobs': -1, 'n_estimators': 70, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 17, 'bootstrap': True},\n",
    "# #         'best_score': 0.8417508417508418\n",
    "# #     },\n",
    "# #     'ada': {\n",
    "# #         'model': AdaBoostClassifier,\n",
    "# #         'params': {\n",
    "# #             'n_estimators': range(10, 251, 20),\n",
    "# #             'learning_rate': [.01, .05, .1, .2, .5, 1, 2],\n",
    "# #             'algorithm': ['SAMME', 'SAMME.R'],\n",
    "# #             'random_state': [SEED],\n",
    "# #         },\n",
    "# #         'best_params': {'random_state': 32, 'n_estimators': 170, 'learning_rate': 1, 'algorithm': 'SAMME.R'},\n",
    "# #         'best_score': 0.8237934904601572\n",
    "# #     },\n",
    "# #     'et': {\n",
    "# #         'model': ExtraTreesClassifier,\n",
    "# #         'params': {\n",
    "# #             'n_estimators': range(10, 251, 20),\n",
    "# #             'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "# #             'max_depth': range(5, 20),\n",
    "# #             'min_samples_split': range(2, 10), # Minimum number of samples required to split a node\n",
    "# #             'min_samples_leaf': range(1, 10), # Minimum number of samples required at each leaf node\n",
    "# #             'bootstrap': [True, False], # Method of selecting samples for training each tree,\n",
    "# #             'random_state': [SEED],\n",
    "# #         },\n",
    "# #         'best_params': {'random_state': 32, 'n_jobs': -1, 'n_estimators': 70, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 11, 'bootstrap': True},\n",
    "# #         'best_score': 0.8294051627384961\n",
    "# #     },\n",
    "# #     'gb': {\n",
    "# #         'model': GradientBoostingClassifier,\n",
    "# #         'params': {\n",
    "# #             'n_estimators': range(10, 251, 20),\n",
    "# #             'max_depth': range(5, 20),\n",
    "# #             'loss': ['deviance', 'exponential'],\n",
    "# #             'learning_rate': [.01, .05, .1, .2, .5, 1, 2],                      \n",
    "# #             'subsample': [.25, .5, .8, 1.],\n",
    "# #             'min_samples_split': range(2, 10), # Minimum number of samples required to split a node\n",
    "# #             'min_samples_leaf': range(1, 10), # Minimum number of samples required at each leaf node\n",
    "# #             'random_state': [SEED],\n",
    "# #         },\n",
    "# #         'best_params': {'subsample': 0.5, 'random_state': 32, 'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 13, 'loss': 'exponential', 'learning_rate': 1},\n",
    "# #         'best_score': 0.8361391694725028\n",
    "# #     }\n",
    "# #     'xgb': {\n",
    "# #         'model': XGBClassifier,\n",
    "# #         'params': {\n",
    "# #             'n_estimators': range(8, 20),\n",
    "# #             'max_depth': range(5, 20),\n",
    "# #             'learning_rate': [.01, .05, .1, .2, .5, 1, 2],\n",
    "# #             'colsample_bytree': [.6, .7, .8, .9, 1]\n",
    "# #         }\n",
    "# #     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# \"\"\" ---------------------------- Best models configuration search --------------------------- \"\"\"\n",
    "\n",
    "# FIT_FROM_SCRATCH = False\n",
    "\n",
    "# for name, model in MODELS.items():\n",
    "    \n",
    "#     if 'best_score' in model and not FIT_FROM_SCRATCH:\n",
    "        \n",
    "#         # Initialize with best parameters & fit to data\n",
    "#         print(f'Fitting {name}...')\n",
    "        \n",
    "#         model['best_estimator'] = model['model'](**model['best_params']).fit(xf_train, y_train)\n",
    "        \n",
    "#         scores = cross_val_score(model['best_estimator'], xf_train, y_train, cv=N_FOLDS)\n",
    "#         score = sum(scores) / len(scores)\n",
    "#         diff = score - model['best_score']\n",
    "        \n",
    "#         if diff > 0:\n",
    "#             print(f'Accuracy of model {name}: {score} (BIGGER for {diff})')\n",
    "#         elif diff < 0:\n",
    "#             print(f'Accuracy of model {name}: {score} (SMALLER for {-diff})')\n",
    "#         else:\n",
    "#             print(f'Accuracy of model {name}: {score} (SAME)')\n",
    "#     else:\n",
    "#         # Perform random search\n",
    "#         searcher = RandomizedSearchCV(param_distributions=model['params'],\n",
    "#                                       estimator=model['model'](), scoring=\"accuracy\",\n",
    "#                                       verbose=1, n_iter=N_ITER, cv=N_FOLDS)\n",
    "#         # Fit to data\n",
    "#         print(f'Fitting {name}...')\n",
    "        \n",
    "#         searcher.fit(xf_train, y_train)\n",
    "\n",
    "#         # Print the best parameters and best accuracy\n",
    "#         print(f'Best parameters found for {name}: {searcher.best_params_}')\n",
    "#         print(f'Best accuracy found {name}: {searcher.best_score_}')\n",
    "\n",
    "#         model['best_estimator'] = searcher.best_estimator_\n",
    "#         model['best_params'] = searcher.best_params_\n",
    "#         model['best_score'] = searcher.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pred = MODELS['knn']['best_estimator'].predict(xf_test)\n",
    "# pred = np.argmax(pred, axis=1)\n",
    "# print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pred = triplet_model.predict(xc_test)\n",
    "# pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'ImageId': range(1, pred.shape[0]+1), 'Label': pred})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
